<!DOCTYPE html>
<html>
  <head>
    <title>Notes - 4 - BioStatistics Boot Camp</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <script type="text/javascript"
	    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <h1>BioStatistics Boot Camp. Section 4</h1>
    <h2>Binomial Proportions</h2>
    <h3>Intervals for binomial proportions</h3>
    <ul>
      <li>When X &sim; Binomial(n, p) we know that
	<ol>
	  <li>\( \hat{p} = X/n\) is the Maximum Likelihood Estimate for p
	  <li>\(E[\hat{p}] = p\)
	  <li>\(Var(\hat{p}) = p(1-p)/n\)
	  <li>\(\frac{\hat{p} - p}{\sqrt{\hat{p}(1-\hat{p})/n}} \) follows
	    a normal distribution for large n
	</ol>
      <li>This last fact leads to the Wald interval for p
	$$ \hat{p} \pm Z_{1-\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n} $$
      <li>The Wald interval performs terribly
      <li>Covarage probability varies wildly, sometimes being quite
	low for certain values of n even when p is not near the
	boundaries (Example: when p = .5 and n = 40, the actual coverage
	of a 95% interval is only 92%)
      <li>When p is small or large, coverage can be quite poor even
	for extremely large values of n. (Example: when p = 0.005 and n
	= 1,876 the actual coverage rate of a 95% interval is only 90%)
    </ul>
    <h3>Agresti-Coull Interval (aka Wilson's Interval)</h3>
    <ul>
      <li>A simple fix for the problem is to add two success and two
	failures. That is, let \( \tilde{p} = (X + 2)/(n+4) \)
      <li>The Agresti-Coull Interval is
	$$ \tilde{p} \pm Z_{1-\alpha/2}\sqrt{\tilde{p}
	(1 - \tilde{p}) / \tilde{n}} $$
      <li>Motivation: when p is large or small, the distribution of
	\(\hat{p}\) is skewed and it does not make sense to center the
	interval at the MLE; adding the pseudo observations pulls the
	center of the interval toward .5
      <li>Later we will show that this interval is the inversion of
	a hypothesis testing technique
    </ul>
    <h3>Example</h3>
    Suppose that in a random sample of an at-risk population 13 of 20
    subjects had hypertension. Estimate the prevalence of hypertension
    in this population.
    <ul>
      <li>\( \hat{p} = 13/20 = .65, n = 20\)
      <li>\( \tilde{p} = 15/24 = .63, n = 24\)
      <li>\( Z_{.975} = 1.96\)
      <li>Wald interval = \(.65 \pm 1.96*\sqrt{.65*.35/20} \) = [.44, .86]
      <li>Agresti-Coull interval = \( .63 \pm 1.96*\sqrt{.63*.37/24}
	\) = [.44, .82]
      <li>1/8 likelihood interval [.42, .84] <i>Calculated how? This
      is not one of the answers given by the binom package I just
      loaded into R</i>
      <li>If \(\hat{p} = p\), then we get
	<ul>
	  <li>\( 13/20 \pm Z_{.975} * s / \sqrt{n} \)
	  <li>\( 13/20 \pm Z_{.975} * \frac{\sqrt{p(1-p)/n}}{\sqrt{n}}\)
	  <li>\( 13/20 \pm Z_{.975} * \frac{\sqrt{p(1-p)}}{n}\)
    </ul>
    <h3>Bayesian analysis</h3>
    <ul>
      <li>Bayesian statistics posits a <b>prior</b> on the parameter
	of interest
      <li>All inferences are then performed on the distribution of the
	parameter given the data, called the <b>posterior</b>
      <li>In general Posterior &propto; Likelihood &times; Prior
      <li>Therefore (as we saw in diagnostic testing) the likelihood
	is the factor by which our prior beliefs are updated to produce
	conclusions in the light of the data
    </ul>
    <h4>Beta priors, Priors Specification</h4>
    <ul>
      <li>The beta distribution is the default prior for paramters
	between 0 and 1.
      <li>The beta density depends on two parameters, &alpha; and
	&beta;
	$$ \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}
	p^{\alpha-1} (1-p)^{\beta - 1} \mathrm{for} 0 \le p \le 1$$ 
      <li>The mean of the beta density is \( \alpha / (\alpha + \beta)
	\)
      <li>The variance of the beta density is \( \frac{\alpha \beta
	}{ (\alpha + \beta)^2(\alpha + \beta + 1)}\)
      <li>The uniform density is the special case where \( \alpha =
	\beta = 1\)
      <li>He shows a set of graphs with alpha and beta at each of 3
	levels, c(.5, 1. 2) which give U's (both are low) or L's (one is
	low), a flat line (both are one), tilted lines (one is 1 and the
	other is two), and parabolas (both are two)
      <li>&alpha; = &beta; = .5 is the <b>Jeffrey's prior</b> which
	has some theoretical benefits
    </ul>
    <h4>Posterior</h4>
    <ul>
      <li>Suppose that we chose values of &alpha; and &beta; so that
	the beta prior is indicative of our degree of belief rearding p
	in the absence of data
      <li>Then using the rule that <i>Posterior &propto; Likelihood
	  &times; Prior</i> and throwing out anything that doesn't depend
	on p, we have that
	$$ \mathrm{Posterior} \varpropto p^x(1-p)^{n-x} \times
	p^{\alpha - 1}(1-p)^{\beta - 1} $$
	$$ = p^{x + \alpha - 1}(1-p)^{n - x + \beta - 1} $$
      <li>This density is just another beta density with paramters
	\(\tilde{\alpha} = x + \alpha\) and \( \tilde{\beta} = n - x + \beta\)
      <li>Posterior mean
	$$ E[p | X] = \frac{\tilde{\alpha}}{\tilde{\alpha} + \tilde{\beta}} $$
	$$  = \frac{x + \alpha}{x + \alpha + n - x + \beta} $$
	$$  = \frac{x + \alpha}{n + \alpha + \beta} $$
	$$  = \frac{x}{n}\times\frac{n}{n + \alpha + \beta} +
	\frac{\alpha}{\alpha + \beta}\times\frac{\alpha + \beta}{n +
	\alpha + \beta} $$
	$$ = \mathrm{MLE} \times \pi + \mathrm{Prior Mean} \times (1 - \pi) $$
      <li>The posterior mean is a mixture of the MLE (\(\hat{p} \) )
	and the prior mean
      <li>&pi; goes to 1 as n gets large; for large n the data swaps
	the prior
      <li>For small n, the prior mean dominates
      <li>Genaralizes how science should ideally work; as data becomes
	increasinglu available, prior beliefs should matter less and
	less
      <li>With a prior that is degenate at a value, no amount of data
	can overcome the prior
      <li>The posterior variance is
	$$ \mathrm{Var}(p | x) = \frac {\tilde{\alpha}\tilde{\beta}}
	{(\tilde{\alpha} + \tilde{\beta})^2 (\tilde{\alpha} +
	\tilde{\beta} + 1)} $$
	$$ = \frac {(x + \alpha)(n - x + \beta} {(n + \alpha + \beta)^ 2 (n +
	\alpha + \beta + 1)} $$
      <li>Let \( \tilde{p} = (x + \alpha)/(n + \alpha + \beta) \) and
	\( \tilde{n} = n + \alpha + \beta \) then we have
	$$ \mathrm{Var}(p | x) = \frac {\tilde{p}(1 - \tilde{p}} {\tilde{n} +
	1} $$
      <li>If &alpha; = &beta; = 2 then the posterior mean is \(
	\tilde{p} = (x + 2)/(n + 4) \) and the posterior variance is \(
	\tilde{p} (1 - \tilde{p}) / (\tilde{n} + 1)\)
      <li>This is almost exactly the mean and varinace we used for the
	Agesti-Coull interval
      <li>He then does some charts showing Prior, Likelihood, and
	Posterior for various alpha and beta settings
	<ul>
	  <li>(.5, .5) prior is a U, likelihood and posterior are
	    very close to each other in a bell curve, mean at about .6
	  <li>(1, 1) prior is a flat line at .5, likelihood and
	    posterior are right on top of each other in a bell curve,
	    mean at about .6
	  <li>(2, 2) prior looks like a parabola, likelihood and posterior are
	    very close to each other in a bell curve, mean at about .6,
	    with the posterior pushed a little bit toward .5 compared to
	    the likelihood
	  <li>(2, 10) prior is a bell centered over .15, posterior is
	    a bell centered over .5, and likelihood is a bell centered
	    over .75.
	  <li>(100, 100) prior and posterior are a skinny bell
	    centered over .5, with the posterior a little higher than
	    the prior, and the likelihood is still a bell over .75. The
	    prior was "Very Sure" and there is lots of data that ought
	    to sway us, and it shoves the posterior a tiny bit up. He
	    likened this to political opinions, which are quite
	    resistant be being changed by data.
	</ul>
    </ul>
    <h4>Credible intervals</h4>
    <ul>
      <li>A Bayesia credible interval is the Bayesian analog of a
	confidence interval
      <li>A 95% credible interval, [a, b] would satisfy \( P(p \in [a,
	b] | x) = .95 \)
      <li>The best credible intervals chop off the posterior with a
	horizontal line in the same way we did for likelihoods
      <li>These are called highest posterior density (HPD) intervals
	<pre>
	  library(binom)
	  binom.bayes(13, 20, type = "highest")
	</pre>
      <li>The command above gives the HPD interval. The default
	credible level is 95% and the default prior is the Jeffrey's prior
    </ul>
    <h4>Interpretation of Confidence Intervals</h4>
    <ul>
      <li>Confidence interval: (Wald) [.44, .86]
      <li>Fuzzy interpretation: We are 95% confident that p lies
	between .44 to .86
      <li>Actual interpretation: The interval .44 to .86 was
	constructed such that in repeated independent experiments, 95%
	of the intervals obtained would contain p.
    </ul>
    <h4>Interpretation of Likelihood Intervals</h4>
    <ul>
      <li>Recall the 1/8 likelihood interval was [.42, .84]
      <li>Fuzzy interpretation: The interval [.42, .84] represents
	plausible values for p
      <li>Actual interpretation: The interval [.42, .84] represents
	plausible values for p in the sense that for each point in this
	interval, there is no other point that is more than 8 times
	better supported given the data
    </ul>
    <h4>Interpretation of Credible Intervals</h4>
    <ul>
      <li>Recall the Jeffrey's prior 95% credible interval was [.44, .84]
      <li>Actual interpretation: The probability that p is between .44
	and .84 is 95%
    </ul>
    <h2>Logs</h2>
    <ul>
      <li>Recall that \( log_B(x) \) is the number y so that \( B^y = x\)
      <li>Note that you cannot take the log of a negative
	number. log(1) is 0 and log(0) is -&infty;
      <li>Recall that log(ab) = log(a) + log(b).
      <li>\(log(a^b) = b log(a) \)
      <li>log(a/b) = log(a) - log(b)
      <li>Reasons to "log" data:
	<ul>
	  <li>To correct for right skewness
	  <li>When considering ratios
	  <li>In settings where errors are feasibly multiplicative,
	    such as when dealing with concentrations or rates
	  <li>To consider orders of magnitude (using log base 10);
	    for example when considering astronomical distances
	  <li>Counts are often logged (though note the problem
	    with zero counts
	</ul>
    </ul>
    <h3>Geometric mean</h3>
    <ul>
      <li>The (sample) <b>geometric mean</b> of a data set X1,...,Xn
	is \( (\prod_{i=1}^n X_i)^{1/n} \)
      <li>Note that (provided that the Xi are positive) the log of
	the geometric mean is \( \frac{1}{n}\sum_{i=1}{n}log(X_i)\)
      <li>As the log of the geometric mean is an average, the Law of
	Large Numbers and Central Limit Theorem apply
      <li>The geometric mean is always less than or equal to the
	sample arethmetic mean.
      <li>The geometric mean is often used when the Xi are all
	multiplicative
      <li>Suppose that in a population of interest, the prevalence
	of a disease rose 2% one year, then fell 1% the next, then
	rose 2%, then rose 1%; since these factors act
	multiplicatively it makes sense to consider the geometric
	mean \( (1.02 \times .99 \times 1.02 \times 1.01)^{1/4} =
	1.01 \) for a geometric mean increase in disease prevalence.
      <li>Note that multiplying the initial prevalence by 1.01^4
	is the same as multiplying the orginal four numbers in
	sequence.
      <li>Hence 1.01 is a constant factor by which you would need
	to multiply the intial prevalence each year to achieve the
	same overall increase in prevalence over a four year period
      <li>The arithmetic mean, in contrast, is the constant factor
	by which you would need to add each year to achieve the same
	total increase. <i>His explanation ignores compounding</i>
      <li>From the question corner at University of Toronto (where
	this example was taken from: If a and b are the lengths of
	the sides of a rectangle then
	<ul>
	  <li>The arithmetic mean (a + b)/2 is the length of the
	    sides of the square that has the same perimter
	  <li>The geometic mean sqrt(a*b) is the length of the
	    sides of the square that has the same area
	</ul>
      <li>Note, by the LLN the log of the geometric mean converges
	to \mu; = E[log(X)]
      <li>Therefore the geometric mean converges to
	\(exp\{E[log(X)]\} = e^\mu\), which is not the population mean
	on the natural scale, we call this the population geometric
	mean (but no one else seems to)
      <li>To reiterate
	$$ \mathrm{exp}\{E[log(x)]\} \ne E[\mathrm{exp}\{log(X)\}] =
	E[X]$$
      <li>Note if the distribution of log(X) is symmetric then \( .5
	= P(logX \le \mu) = P(X \le e^\mu) \)
      <li>Therefore, for log-symmetric distributions the geometric
	mean is estimating the median
    </ul>
    <h3>Geometric means and the Central Limit Theorem</h3>
    <ul>
      <li>If you use the CLT to create a confidence interval for the
	log measurements, your interval is estimating &mu;, the expected
	value of the log measurements
      <li>If you exponentiate the endpoints of the interval, you are
	estimating \(e^\mu\), the population geometric mean
      <li>Recall, \( e^\mu\) is the population median when the
	distribution of the logged data is symmetric
      <li>This is especially useful for paired data when their
	ratio, rather than their difference, is of interest
      <li>Gives long explanation of what a paired design is. In this
	particular case, there is a study of people taking Oral
	Contraceptives, trying to figure out if there is a change in
	Systolic Blood Pressure. The pairing comes in where you think
	there might be a systematic difference between women who take
	OC and women who don't, so you choose your control group
	carefully to match the OC group in those ways you think the
	groups might differ naturally.
    </ul>
    <h3>Comparisons</h3>
    <ul>
      <li>Consider when you have two independent groups, logging the
	individual data points and creating a confidence interval for
	the difference in the log means
      <li>Prove to yourself that exponentiating the endpoints of
	this interval is then an interval for the ratio of the
	population geometric means, \( \frac{e^{\mu_1}}{e^{\mu_2}}\)
    </ul>
    <h3>The log-normal distribution</h3>
    <ul>
      <li>A random variable is <b>log-normally</b> distributed if its
	log is a normally distributed random variable
      <li>"I am log-normal" means "take logs of me and then I'll be
	normal"
      <li>Note log-normal random variables are not logs of normal
	random variables!!
      <li>Formally, X is lognormal(\(\mu, \sigma^2\) ) if log\((X)
	\sim N(\mu, \sigma^2) \)
      <li>If \( Y \sim N(\mu, \sigma^2) \) then \(X = e^Y\) is
	log-normal
      <li>The log-normal density is
	$$ \frac{1} {\sqrt{2\pi}} \times
	\frac{\mathrm{exp}[-log\{log(x) - \mu\}^2/(2\sigma^2)]} {x}
	\mathrm{for} 0 \le x \le \infty $$
      <li>Its mean is \( e^{\mu + \sigma^2/2\) and variance is \(
	e^{2\mu + \sigma^2}(e^{\sigma^2} - 1) \)
      <li>Its median is \( e^\mu \)
      <li>Notice that if we assume that X1,...,Xn are
	log-normal(\(\mu, \sigma^2\)) then Y1 = log(X1), ... , Yn =
	log(Xn) are normally distributed with mean &mu; and variance
	\(\sigma^2\)
      <li>Creating a Gosset's t confidence interval on using the Yi
	is a confidence interval for &mu; the log of the median of the
	Xi
      <li>Exponentiate the endpoints of the interval to obtain a
	confidence interval for \(e^\mu\), the median on the original
	scale.
      <li>Assuming log-normality, exponentiating t confidence
	intervals for the difference in two log means again estimates
	ratios of geometric means
    </ul>
  </body>
</html>
