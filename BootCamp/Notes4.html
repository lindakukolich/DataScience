<!DOCTYPE html>
<html>
  <head>
    <title>Notes - 3 - BioStatistics Boot Camp</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <script type="text/javascript"
	    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <h1>BioStatistics Boot Camp. Section 4</h1>
    <h2>Binomial Proportions</h2>
    <h3>Intervals for binomial proportions</h3>
    <ul>
      <li>When X &tilde; Binomial(n, p) we know that
	<ol>
	  <li>\( \hat{p} = X/n\) is the Maximum Likelihood Estimate for p
	  <li>\(E[\hat{p}] = p\)
	  <li>\(Var(\hat{p}) = p(1-p)/n\)
	  <li>\(\frac{\hat{p} - p}{\sqrt{\hat{p}(1-\hat{p})/n}} \) follows
	    a normal distribution for large n
	</ol>
      <li>This last fact leads to the Wald interval for p
	$$ \hat{p} \pm Z_{1-\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n} $$
      <li>The Wald interval performs terribly
      <li>Covarage probability varies wildly, sometimes being quite
      low for certain values of n even when p is not near the
      boundaries (Example: when = - .5 and n = 40, the actual coverate
      of a 95% interval is only 92%)
      <li>When p is small or large, coverage can be quite poor even
      for extremely large values of n. (Example: when p = 0.005 and n
      = 1,876 the actual coverage rate of a 95% interval is only 90%)
    </ul>
    <h3>Agresti-Coull Interval</h3>
    <ul>
      <li>A simple fix for the problem is to add two success and two
	failures. That is, let \( \tilde{p} = (X + 2)/(n+4) \)
	<li>The Agresti-Coull Interval is
	  $$ \tilde{p} \pm Z_{1-\alpha/2}\sqrt{\tilde{p}
	  (1 - \tilde{p}) / \tilde{n}} $$
	<li>Motivation: when p is large or small, the distribution of
	\(\hat{p}\) is skewed and it does not make sense to center the
	interval at the MLE; adding the pseudo observations pulls the
	center of the interval toward .5
	<li>Later we will show that this interval is the inversion of
	a hypothesis testing technique
    </ul>
    <h3>Example</h3>
    Suppose that in a random sample of an at-risk population 13 of 20
    subjects had hypertension. Estimate the prevalence of hypertension
    in this population.
    <ul>
      <li>\( \hat{p} = 13/20 = .65, n = 20\)
      <li>\( \tilde{p} = 15/24 = .63, n = 24\)
      <li>\( Z_{.975} = 1.96\)
      <li>Wald interval = \(.65 \pm 1.96*\sqrt{.65*.35/20} \) = [.44, .86]
      <li>Agresti-Coull interval = \( .63 \pm 1.96*\sprt{.63*.37/24}
      \) = [.44, .82]
      <li>1/8 likelihood interval [.42, .84] <i>I have no idea how to
      calculate this</i>
    </ul>
    <h3>Bayesian analysis</h3>
    <ul>
      <li>Bayesian statistics posits a <b>prior</b> on the parameter
      of interest
      <li>All inferences are then performed on the distribution of the
      parameter given the data, called the <b>posterior</b>
      <li>In general Posterior &propto; Likeelihood &times; Prior
      <li>Therefore (as we saw in diagnostic testing) the likelihood
      is the factor by which our prior beliefs are updated to produce
      conclusions in the light of the data
    </ul>
    <h4>Beta priors, Priors Specification</h4>
    <ul>
      <li>The beta distribution is the default prior for paramters
      between 0 and 1.
      <li>The beta density depends on two parameters, &alpha; and
      &beta;
	$$ \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}
      p^{\alpha-1} (1-p)^{\beta - 1} \mathrm{for} 0 \le p \le 1$$ 
      <li>The mean of the beta density is \( \alpha / (\alpha + \beta)
      \)
      <li>The variance of the beta density is \( \frac{\alpha \beta
	}{ (\alpha + \beta)^2(\alpha + \beta + 1)}\)
      <li>The uniform density is the special case where \( \alpha =
      \beta = 1\)
      <li>He shows a set of graphs with alpha and beta at each of 3
      levels, c(.5, 1. 2) which give U's (both are low) or L's (one is
      low), a flat line (both are one), tilted lines (one is 1 and the
      other is two), and parabolas (both are two)
      <li>&alpha; = &beta; = .5 is the <b>Jeffrey's prior</b> which
      has some theoretical benefits
    </ul>
    <h4>Posterior</h4>
    <ul>
      <li>Suppose that we chose values of &alpha; and &beta; so that
      the beta prior is indicative of our dgree of belief rearding p
      in the absence of data
      <li>Then using the rule that <i>Posterior &propto; Likelihood
      &times; Prior</i> and throwing out anything that doesn't depend
      on p, we have that
	$$ \mathrm{Posterior} \varpropto p^x(1-p)^{n-x} \times
      p^{\alpha - 1}(1-p)^{\beta - 1} $$
	$$ = p^{x + \alpha - 1}(1-p)^{n - x + \beta - 1} $$
      <li>This density is just another beta density with paramters
      \(\tilde{\alpha} = x + \alpha\) and \( \tilde{\beta} = n - x + \beta\)
      <li>Posterior mean
$$ E[p | X] = \frac{\tilde{\alpha}}{\tilde{\alpha} + \tilde{\beta}} $$
$$  = \frac{x + \alpha}{x + \alpha + n - x + \beta} $$
$$  = \frac{x + \alpha}{n + \alpha + \beta} $$
$$  = \frac{x}{n}\times\frac{n}{n + \alpha + \beta} +
	\frac{\alpha}{\alpha + \beta}\times\frac{\alpha + \beta}{n +
	\alpha + \beta} $$
$$ = \mathrm{MLE} \times \pi + \mathrm{Prior Mean} \times (1 - \pi) $$
      <li>The posterior mean is a mixture of the MLE (\(\hat{p} \) )
      and the prior mean
      <li>&pi; goes to 1 as n gets large; for large n the data swaps
      the prior
      <li>For small n, the prior mean dominates
      <li>Genaralizes how science should ideally work; as data becomes
      increasinglu available, prior beliefs should matter less and
      less
      <li>With a prior that is degenate at a value, no amount of data
      can overcome the prior
      <li>The posterior variance is
$$ \mathrm{Var}(p | x) = \frac {\tilde{\alpha}\tilde{\beta}}
	{(\tilde{\alpha} + \tilde{\beta})^2 (\tilde{\alpha} +
	\tilde{\beta} + 1)} $$
$$ = \frac {(x + \alpha)(n - x + \beta} {(n + \alpha + \beta)^ 2 (n +
	\alpha + \beta + 1)} $$
      <li>Let \( \tilde{p} = (x + \alpha)/(n + \alpha + \beta) \) and
      \( \tilde{n} = n + \alpha + \beta \) then we have
$$ \mathrm{Var}(p | x) = \frac {\tilde{p}(1 - \tilde{p}} {\tilde{n} +
      1} $$
      <li>If &alpha; = &beta; = 2 then the posterior mean is \(
      \tilde{p} = (x + 2)/(n + 4) \) and the posterior variance is \(
      \tilde{p} (1 - \tilde{p}) / (\tilde{n} + 1)\)
      <li>This is almost exactly the mean and varinace we used for the
      Agesti-Coull interval
      <li>He then does some charts showing Prior, Likelihood, and
      Posterior for various alpha and beta settings
	<ul>
	  <li>(.5, .5) prior is a U, likelihood and posterior are
	  very close to each other in a bell curve, mean at about .6
	  <li>(1, 1) prior is a flat line at .5, likelihood and
	  posterior are right on top of each other in a bell curve,
	  mean at about .6
	  <li>(2, 2) prior looks like a parabola, likelihood and posterior are
	  very close to each other in a bell curve, mean at about .6,
	  with the likelihood a little higher mean than the posterior
	  <li>(2, 10) prior is a bell centered over .15, posterior is
	  a bell centered over .5, and likelihood is a bell centerd
	  over .75
	  <li>(100, 100) prior and posterior are a skinny bell
	  centered over .5, with the posterior a little higher than
	  the prior, and the likelihood is still a bell over .75.
	</ul>
    </ul>
    <h4>Credible intervals</h4>
    <ul>
      <li>A Bayesia credible interval is the Bayesian analog of a
      confidence interval
      <li>A 95% credible interval, [a, b] would satisfy \( P(p \in [a,
      b] | x) = .95 \)
      <li>The best credible intervals chop off the posterior with a
      horizontal line in the same way we did for likelihoods
      <li>These are called highest posterior density (HPD) intervals
<pre>
library(binom)
binom.bayes(13, 20, type = "highest")
</pre>
      <li>The command above gives the HPD interval. The default
      credible level is 95% and the default prior is the Jeffrey's prior
    </ul>
    <h4>Interpretation of Confidence Intervals</h4>
    <ul>
      <li>Confidence interval: (Wald) [.44, .86]
      <li>Fuzzy interpretation: We are 95% confident that p lies
      between .44 to .86
      <li>Actual interpretation: The interval .44 to .86 was
      constructed such that in repeated independent experiments, 95%
      of the intervals obtained would contain p.
    </ul>
    <h4>Interpretation of Likelihood Intervals</h4>
    <ul>
      <li>Recall the 1/8 likelihood interval was [.42, .84]
      <li>Fuzzy interpretation: The interval [.42, .84] represents
      plausible values for p
      <li>Actual interpretation: The interval [.42, .84] represents
      plausible values for p in the sense that for each point in this
      interval, there is no other point that is more than 8 times
      better supported given the data
    </ul>
    <h4>Interpretation of Credible Intervals</h4>
    <ul>
      <li>Recall the Jeffrey's prior 95% credible interval was [.44, .84]
      <li>Actual interpretation: The probability that p is between .44
      and .84 is 95%
    </ul>
    <h2>Logs</h2>
    <h3>Geometric mean</h3>
    <h3>GM and the Central Limit Theorem</h3>
    <h3>Comparisons</h3>
    <h3>The log-normal distribution</h3>
  </body>
</html>
