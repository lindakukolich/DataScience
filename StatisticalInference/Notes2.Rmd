
title: "Notes 2 - Statistical Inference"
author: "Linda Kukolich"
date: "February 10, 2015"
output: html_document


***These notes are a copy of the original slides***

## The variance

- The variance of a random variable is a measure of *spread*
- If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as

$$
Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2
$$ 

- The expected (squared) distance from the mean
- Densities with a higher variance are more spread out than densities with a lower variance
- The square root of the variance is called the **standard deviation**
- The standard deviation has the same units as $X$



## Example

- What's the variance from the result of a toss of a die? 

  - $E[X] = 3.5$ 
  - $E[X^2] = 1 ^ 2 \times \frac{1}{6} + 2 ^ 2 \times \frac{1}{6} + 3 ^ 2 \times \frac{1}{6} + 4 ^ 2 \times \frac{1}{6} + 5 ^ 2 \times \frac{1}{6} + 6 ^ 2 \times \frac{1}{6} = 15.17$ 

- $Var(X) = E[X^2] - E[X]^2 \approx 2.92$



## Example

- What's the variance from the result of the toss of a coin with probability of heads (1) of $p$? 

  - $E[X] = 0 \times (1 - p) + 1 \times p = p$
  - $E[X^2] = E[X] = p$ 

$$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$$



## Distributions with increasing variance
```{r, echo = FALSE, fig.height = 6, fig.width = 8, fig.align='center'}
library(ggplot2)
xvals <- seq(-10, 10, by = .01)
dat <- data.frame(
    y = c(
        dnorm(xvals, mean = 0, sd = 1),
        dnorm(xvals, mean = 0, sd = 2),
        dnorm(xvals, mean = 0, sd = 3),
        dnorm(xvals, mean = 0, sd = 4)
    ),
    x = rep(xvals, 4),
    factor = factor(rep(1 : 4, rep(length(xvals), 4)))
)
ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)    
```


## The sample variance 
- The sample variance is 
$$
S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}
$$
(almost, but not quite, the average squared deviation from
the sample mean)
- It is also a random variable
  - It has an associate population distribution
  - Its expected value is the population variance
  - Its distribution gets more concentrated around the population variance with more data
- Its square root is the sample standard deviation



## Simulation experiment
### Simulating from a population with variance 1

```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
library(ggplot2)
nosim <- 10000; 
dat <- data.frame(
    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),
          apply(matrix(rnorm(nosim * 20), nosim), 1, var),
          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),
    n = factor(rep(c("10", "20", "30"), c(nosim, nosim, nosim))) 
    )
ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 2, alpha = .2) + geom_vline(xintercept = 1, size = 2) 

```


## Variances of x die rolls
```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}  
dat <- data.frame(
  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), 
                     nosim), 1, var),
        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), 
                     nosim), 1, var),
        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), 
                     nosim), 1, var)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black") 
g <- g + geom_vline(xintercept = 2.92, size = 2)
g + facet_grid(. ~ size)
```




## Recall the mean
- Recall that the average of random sample from a population 
is itself a random variable
- We know that this distribution is centered around the population
mean, $E[\bar X] = \mu$
- We also know what its variance is $Var(\bar X) = \sigma^2 / n$
- This is very useful, since we don't have repeat sample means 
to get its variance; now we know how it relates to
the population variance
- **We call the standard deviation of a statistic a standard error**


## To summarize
- The sample variance, $S^2$, estimates the population variance, $\sigma^2$
- The distribution of the sample variance is centered around $\sigma^2$
- The the variance of sample mean is $\sigma^2 / n$
  - Its logical estimate is $s^2 / n$
  - The logical estimate of the standard error is $s / \sqrt{n}$
- $s$, the standard deviation, talks about how variable the population is
- $s/\sqrt{n}$, the standard error, talks about how variable averages of random samples of size $n$ from the population are


## Simulation example
Standard normals have variance 1; means of $n$ standard normals
have standard deviation $1/\sqrt{n}$

```{r}
nosim <- 1000
n <- 10
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
1 / sqrt(n)
```



## Simulation example
Standard uniforms have variance $1/12$; means of 
random samples of $n$ uniforms have sd $1/\sqrt{12 \times n}$


```{r}
nosim <- 1000
n <- 10
sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
1 / sqrt(12 * n)
```



## Simulation example
Poisson(4) have variance $4$; means of 
random samples of $n$ Poisson(4) have sd $2/\sqrt{n}$


```{r}
nosim <- 1000
n <- 10
sd(apply(matrix(rpois(nosim * n, 4), nosim), 1, mean))
2 / sqrt(n)
```



## Simulation example
Fair coin flips have variance $0.25$; means of 
random samples of $n$ coin flips have sd $1 / (2 \sqrt{n})$


```{r}
nosim <- 1000
n <- 10
sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE),
                nosim), 1, mean))
1 / (2 * sqrt(n))
```


## Data example
```{r}
library(UsingR); data(father.son); 
x <- father.son$sheight
n<-length(x)
```


## Plot of the son's heights
```{r, fig.height=6, fig.width=6, echo=FALSE, fig.align='center'}
g <- ggplot(data = father.son, aes(x = sheight)) 
g <- g + geom_histogram(aes(y = ..density..), fill = "lightblue", binwidth=1, colour = "black")
g <- g + geom_density(size = 2, colour = "black")
g
```


## Let's interpret these numbers
```{r}
round(c(var(x), var(x) / n, sd(x), sd(x) / sqrt(n)),2)
# variance (in^2), se^2, standard deviation (in), standard error (standard deviation in the distributon of averages of n children's heights)
```

```{r, echo = FALSE, fig.height=4, fig.width=4,fig.align='center'}
g
```



## Summarizing what we know about variances
- The sample variance estimates the population variance
- The distribution of the sample variance is centered at
what its estimating
- It gets more concentrated around the population variance with larger sample sizes
- The variance of the sample mean is the population variance
divided by $n$
  - The square root is the standard error
- It turns out that we can say a lot about the distribution of
averages from random samples, 
even though we only get one to look at in a given data set

# Common distributions

## The Bernoulli distribution

- The **Bernoulli distribution** arises as the result of a binary outcome
- Bernoulli random variables take (only) the values 1 and 0 with probabilities of (say) $p$ and $1-p$ respectively
- The PMF for a Bernoulli random variable $X$ is $$P(X = x) =  p^x (1 - p)^{1 - x}$$
- The mean of a Bernoulli random variable is $p$ and the variance is $p(1 - p)$
- If we let $X$ be a Bernoulli random variable, it is typical to call $X=1$ as a "success" and $X=0$ as a "failure"




## Binomial trials

- The *binomial random variables* are obtained as the sum of iid Bernoulli trials
- In specific, let $X_1,\ldots,X_n$ be iid Bernoulli$(p)$; then $X = \sum_{i=1}^n X_i$ is a binomial random variable
- The binomial mass function is
$$
P(X = x) = 
\left(
\begin{array}{c}
  n \\ x
\end{array}
\right)
p^x(1 - p)^{n-x}
$$
for $x=0,\ldots,n$



## Choose

- Recall that the notation 
  $$\left(
    \begin{array}{c}
      n \\ x
    \end{array}
  \right) = \frac{n!}{x!(n-x)!}
  $$ (read "$n$ choose $x$") counts the number of ways of selecting $x$ items out of $n$
  without replacement disregarding the order of the items

$$\left(
    \begin{array}{c}
      n \\ 0
    \end{array}
  \right) =
\left(
    \begin{array}{c}
      n \\ n
    \end{array}
  \right) =  1
  $$ 



## Example

- Suppose a friend has $8$ children (oh my!), $7$ of which are girls and none are twins
- If each gender has an independent $50$% probability for each birth, what's the probability of getting $7$ or more girls out of $8$ births?

$$
{8 \choose 7} .5^{7}(1-.5)^{1}
+
{8 \choose 8} .5^{8}(1-.5)^{0} \approx 0.04
$$
```{r}
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8 
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```




## The normal distribution

- A random variable is said to follow a **normal** or **Gaussian** distribution with mean $\mu$ and variance $\sigma^2$ if the associated density is
  $$
  (2\pi \sigma^2)^{-1/2}e^{-(x - \mu)^2/2\sigma^2}
  $$
  If $X$ a RV with this density then $E[X] = \mu$ and $Var(X) = \sigma^2$
- We write $X\sim \mbox{N}(\mu, \sigma^2)$
- When $\mu = 0$ and $\sigma = 1$ the resulting distribution is called **the standard normal distribution**
- Standard normal RVs are often labeled $Z$


## The standard normal distribution with reference lines 
```{r, fig.height=6, fig.width=6, fig.align='center', echo = FALSE}
x <- seq(-3, 3, length = 1000)
library(ggplot2)
g <- ggplot(data.frame(x = x, y = dnorm(x)), 
            aes(x = x, y = y)) + geom_line(size = 2)
g <- g + geom_vline(xintercept = -3 : 3, size = 2)
g
```



## Facts about the normal density

If $X \sim \mbox{N}(\mu,\sigma^2)$ then 
$$Z = \frac{X -\mu}{\sigma} \sim N(0, 1)$$ 


If $Z$ is standard normal $$X = \mu + \sigma Z \sim \mbox{N}(\mu, \sigma^2)$$



## More facts about the normal density

1. Approximately $68\%$, $95\%$ and $99\%$  of the normal density lies within $1$, $2$ and $3$ standard deviations from the mean, respectively
2. $-1.28$, $-1.645$, $-1.96$ and $-2.33$ are the $10^{th}$, $5^{th}$, $2.5^{th}$ and $1^{st}$ percentiles of the standard normal distribution respectively
3. By symmetry, $1.28$, $1.645$, $1.96$ and $2.33$ are the $90^{th}$, $95^{th}$, $97.5^{th}$ and $99^{th}$ percentiles of the standard normal distribution respectively



## Question

- What is the $95^{th}$ percentile of a $N(\mu, \sigma^2)$ distribution? 
  - Quick answer in R `qnorm(.95, mean = mu, sd = sd)`
- Or, because you have the standard normal quantiles memorized
and you know that 1.645 is the 95th percentile you know that the answer has to be
$$\mu + \sigma 1.645$$
- (In general $\mu + \sigma z_0$ where $z_0$ is the appropriate standard normal quantile)



## Question

- What is the probability that a $\mbox{N}(\mu,\sigma^2)$ RV is larger than $x$?

'pnorm(x, mean=mu, sd = sigma, lower.tail=FALSE)`
OR
'1- pnorm(x, mean=mu, sd=sigma)'
$\frac{x - \mu}{\sigma}$

## Example

Assume that the number of daily ad clicks for a company 
is (approximately) normally distributed with a mean of 1020 and a standard
deviation of 50. What's the probability of getting
more than  1,160 clicks in a day?

## Example

Assume that the number of daily ad clicks for a company 
is (approximately) normally distributed with a mean of 1020 and a standard
deviation of 50. What's the probability of getting
more than  1,160 clicks in a day?

It's not very likely, 1,160 is `r (1160 - 1020) / 50` standard
deviations from the mean 
```{r}
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm((1160-1020)/50, lower.tail = FALSE)
```



## Example

Assume that the number of daily ad clicks for a company 
is (approximately) normally distributed with a mean of 1020 and a standard
deviation of 50. What number of daily ad clicks would represent
the one where 75% of days have fewer clicks (assuming
days are independent and identically distributed)?



## Example

Assume that the number of daily ad clicks for a company 
is (approximately) normally distributed with a mean of 1020 and a standard
deviation of 50. What number of daily ad clicks would represent
the one where 75% of days have fewer clicks (assuming
days are independent and identically distributed)?

We know 50% is at 1020. We know that 1 sigma = 68%, so 16% lies above 1 sigma (1020 + 50) (the 84% mark)
```{r}
qnorm(0.75, mean = 1020, sd = 50)
```


## The Poisson distribution
* Used to model counts
* The Poisson mass function is
$$
P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
for $x=0,1,\ldots$
* The mean of this distribution is $\lambda$
* The variance of this distribution is $\lambda$
* Notice that $x$ ranges from $0$ to $\infty$


## Some uses for the Poisson distribution
* Modeling count data
    - model unbounded count data
* Modeling event-time or survival data
    - Whether someone has recurrance of symptoms in a cancer study. If someone hasn't shown symptoms within the time of study, that is not the same as remission.
* Modeling radioactive decay, survival data, unbounded count data
* Modeling contingency tables
    - counts of something like hair color by race
* Approximating binomials when $n$ is large and $p$ is small

## Poisson derivation
- $\lambda$ is the mean number of events per unit time
- Let h be very small
- Suppose we assume that
    - Prob of an event in an interval of length is $\lambda h$ while the prob. of more than one is neglibible
    - Whether or not an event occurs in one small inverval does not impact whether or not an event occurs in another small interval then, the number of events per unit time is Poisson with rate $\lambda t$
    
## Rates and Poisson random variables
* Poisson random variables are used to model rates
* $X \sim Poisson(\lambda t)$ where 
  * $\lambda = E[X / t]$ is the expected count per unit of time
  * $t$ is the total monitoring time


## Example
The number of people that show up at a bus stop is Poisson with
a mean of $2.5$ per hour.

If watching the bus stop for 4 hours, what is the probability that $3$
or fewer people show up for the whole time?

```{r}
ppois(3, lambda = 2.5 * 4)
```


## Poisson approximation to the binomial
* When $n$ is large and $p$ is small the Poisson distribution
  is an accurate approximation to the binomial distribution
* Notation
  * $X \sim \mbox{Binomial}(n, p)$
  * $\lambda = n p$
  * $n$ gets large 
  * $p$ gets small

## Example, Poisson approximation to the binomial

We flip a coin with success probablity $0.01$ five hundred times. 

What's the probability of 2 or fewer successes?

```{r}
pbinom(2, size = 500, prob = .01)
ppois(2, lambda=500 * .01)
```

## Asymptotics
* Asymptotics is the term for the behavior of statistics as the sample size (or some other relevant quantity) limits to infinity (or some other relevant number)
* (Asymptopia is my name for the land of asymptotics, where everything works out well and there's no messes. The land of infinite data is nice that way.)
* Asymptotics are incredibly useful for simple statistical inference and approximations 
* (Not covered in this class) Asymptotics often lead to nice understanding of procedures
* Asymptotics generally give no assurances about finite sample performance
* Asymptotics form the basis for frequency interpretation of probabilities 
  (the long run proportion of times an event occurs)




## Limits of random variables

- Fortunately, for the sample mean there's a set of powerful results
- These results allow us to talk about the large sample distribution
of sample means of a collection of $iid$ observations
- The first of these results we inuitively know
  - It says that the average limits to what its estimating, the population mean
  - It's called the Law of Large Numbers
  - Example $\bar X_n$ could be the average of the result of $n$ coin flips (i.e. the sample proportion of heads)
    - As we flip a fair coin over and over, it evetually converges to the
    true probability of a head
    The LLN forms the basis of frequency style thinking



## Law of large numbers in action
```{r, fig.height=5, fig.width=5}
n <- 10000; means <- cumsum(rnorm(n)) / (1  : n); library(ggplot2)
g <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) 
g <- g + geom_hline(yintercept = 0) + geom_line(size = 2) 
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
```



## Law of large numbers in action, coin flip
```{r, fig.height=5, fig.width=5}
means <- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)
g <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) 
g <- g + geom_hline(yintercept = 0.5) + geom_line(size = 2) 
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
```




## Discussion
- An estimator is **consistent** if it converges to what you want to estimate
  - The LLN says that the sample mean of iid sample is
  consistent for the population mean
  - Typically, good estimators are consistent; it's not too much to ask that if we go to the trouble of collecting an infinite amount of data that we get the right answer
- The sample variance and the sample standard deviation
of iid random variables are consistent as well



## The Central Limit Theorem

- The **Central Limit Theorem** (CLT) is one of the most important theorems in statistics
- For our purposes, the CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases
- The CLT applies in an endless variety of settings
- The result is that 
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=
\frac{\sqrt n (\bar X_n - \mu)}{\sigma}
= \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}$$ has a distribution like that of a standard normal for large $n$.
- (Replacing the standard error by its estimated value doesn't change the CLT)
- The useful way to think about the CLT is that 
$\bar X_n$ is approximately
$N(\mu, \sigma^2 / n)$ (population mean and standard error of the mean)





## Example

- Simulate a standard normal random variable by rolling $n$ (six sided)
- Let $X_i$ be the outcome for die $i$
- Then note that $\mu = E[X_i] = 3.5$
- $Var(X_i) = 2.92$ 
- SE $\sqrt{2.92 / n} = 1.71 / \sqrt{n}$
- Lets roll $n$ dice, take their mean, subtract off 3.5,
and divide by $1.71 / \sqrt{n}$ and repeat this over and over



## Result of our die rolling experiment

```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71
dat <- data.frame(
  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```



## Coin CLT

 - Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin
- The sample proportion, say $\hat p$, is the average of the coin flips
- $E[X_i] = p$ and $Var(X_i) = p(1-p)$
- Standard error of the mean is $\sqrt{p(1-p)/n}$
- Then
$$
    \frac{\hat p - p}{\sqrt{p(1-p)/n}}
$$
will be approximately normally distributed
- Let's flip a coin $n$ times, take the sample proportion
of heads, subtract off .5 and multiply the result by
$2 \sqrt{n}$ (divide by $1/(2 \sqrt{n})$)


## Simulation results
```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5) 
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```


## Simulation results, $p = 0.9$
```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```


## Galton's quincunx 
```{r}
if (!file.exists("fig")){
    dir.create("fig")
}
if (!file.exists("fig/quincunx.png")) {
    download.file("http://en.wikipedia.org/wiki/Bean_machine#mediaviewer/File:Quincunx_(Galton_Box)_-_Galton_1889_diagram.png", "fig/quincunx.png")
}
```
http://en.wikipedia.org/wiki/Bean_machine#mediaviewer/File:Quincunx_(Galton_Box)_-_Galton_1889_diagram.png

<img src="fig/quincunx.png" height="450"></img>



## Confidence intervals

- According to the CLT, the sample mean, $\bar X$, 
is approximately normal with mean $\mu$ and sd $\sigma / \sqrt{n}$
- $\mu + 2 \sigma /\sqrt{n}$ is pretty far out in the tail
(only 2.5% of a normal being larger than 2 sds in the tail)
- Similarly, $\mu - 2 \sigma /\sqrt{n}$ is pretty far in the left tail (only 2.5% chance of a normal being smaller than 2 sds in the tail)
- So the probability $\bar X$ is bigger than $\mu + 2 \sigma / \sqrt{n}$
or smaller than $\mu - 2 \sigma / \sqrt{n}$ is 5%
    - Or equivalently, the probability of being between these limits is 95%
- The quantity $\bar X \pm 2 \sigma /\sqrt{n}$ is called
a 95% interval for $\mu$
- The 95% refers to the fact that if one were to repeatly
get samples of size $n$, about 95% of the intervals obtained
would contain $\mu$
- The 97.5th quantile is 1.96 (so I rounded to 2 above)
- 90% interval you want (100 - 90) / 2 = 5% in each tail 
  - So you want the 95th percentile (1.645)



## Give a confidence interval for the average height of sons
in Galton's data
```{r}
library(UsingR);data(father.son); x <- father.son$sheight
(mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12
```



## Sample proportions

- In the event that each $X_i$ is $0$ or $1$ with common success probability $p$ then $\sigma^2 = p(1 - p)$
- The interval takes the form
$$
    \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}
$$
- Replacing $p$ by $\hat p$ in the standard error results in what is called a Wald confidence interval for $p$
- For 95% intervals
$$\hat p \pm \frac{1}{\sqrt{n}}$$ 
is a quick CI estimate for $p$


## Example
* Your campaign advisor told you that in a random sample of 100 likely voters,
  56 intend to vote for you. 
  * Can you relax? Do you have this race in the bag?
  * Without access to a computer or calculator, how precise is this estimate?
* `1/sqrt(100)=0.1` so a back of the envelope calculation gives an approximate 95% interval of `(0.46, 0.66)`
  * Not enough for you to relax, better go do more campaigning!
* Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.
```{r}
round(1 / sqrt(10 ^ (1 : 6)), 3)
```




## Binomial interval

```{r}
.56 + c(-1, 1) * qnorm(.975) * sqrt(.56 * .44 / 100)
binom.test(56, 100)$conf.int
```



## Simulation

```{r}
n <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage <- sapply(pvals, function(p){
  phats <- rbinom(nosim, prob = p, size = n) / n
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})

```



## Plot of the results (not so good)
```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
````


## What's happening?
- $n$ isn't large enough for the CLT to be applicable
for many of the values of $p$
- Quick fix, form the interval with 
$$
\frac{X + 2}{n + 4}
$$
- (Add two successes and failures, Agresti/Coull interval)


## Simulation
First let's show that coverage gets better with $n$

```{r}
n <- 100; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage2 <- sapply(pvals, function(p){
  phats <- rbinom(nosim, prob = p, size = n) / n
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})

```


## Plot of coverage for $n=100$
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage2)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)
```


## Simulation
Now let's look at $n=20$ but adding 2 successes and failures
```{r}
n <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage <- sapply(pvals, function(p){
  phats <- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})
```



## Adding 2 successes and 2 failures
(It's a little conservative)
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)
````



## Poisson interval
* A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
* $X \sim Poisson(\lambda t)$.
* Estimate $\hat \lambda = X/t$
* $Var(\hat \lambda) = \lambda / t$ 
* $\hat \lambda / t$ is our variance estimate


## R code
```{r}
x <- 5; t <- 94.32; lambda <- x / t
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
poisson.test(x, T = 94.32)$conf
```



## Simulating the Poisson coverage rate
Let's see how this interval performs for lambda
values near what we're estimating
```{r}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 100
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  # Find the number of times the true lambda was withing the estimated limits
  mean(ll < lambda & ul > lambda)
})
```




## Covarage
(Gets really bad for small values of lambda)
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)
````




## What if we increase t to 1000?
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 1000
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)
```

Better coverage, except for small values of lambda.


## Summary
- The LLN states that averages of iid samples 
converge to the population means that they are estimating
- Poisson rates converge to the rates they are estimating
- The CLT states that averages are approximately normal, with
distributions
  - centered at the population mean 
  - with standard deviation equal to the standard error of the mean
  - CLT gives no guarantee that $n$ is large enough
- Taking the mean and adding and subtracting the relevant
normal quantile times the SE yields a confidence interval for the mean
  - Adding and subtracting 2 SEs works for 95% intervals
- Confidence intervals get wider as the coverage increases
(why?)
- Confidence intervals get narrower with less variability or
larger sample sizes
- The Poisson and binomial case have exact intervals that
don't require the CLT
  - But a quick fix for small sample size binomial calculations is to add 2 successes and failures

## Asymptotics
* Asymptotics is the term for the behavior of statistics as the sample size (or some other relevant quantity) limits to infinity (or some other relevant number)
* (Asymptopia is my name for the land of asymptotics, where everything works out well and there's no messes. The land of infinite data is nice that way.)
* Asymptotics are incredibly useful for simple statistical inference and approximations 
* (Not covered in this class) Asymptotics often lead to nice understanding of procedures
* Asymptotics generally give no assurances about finite sample performance
* Asymptotics form the basis for frequency interpretation of probabilities 
  (the long run proportion of times an event occurs)




## Limits of random variables

- Fortunately, for the sample mean there's a set of powerful results
- These results allow us to talk about the large sample distribution
of sample means of a collection of $iid$ observations
- The first of these results we inuitively know
  - It says that the average limits to what its estimating, the population mean
  - It's called the Law of Large Numbers
  - Example $\bar X_n$ could be the average of the result of $n$ coin flips (i.e. the sample proportion of heads)
    - As we flip a fair coin over and over, it evetually converges to the
    true probability of a head
    The LLN forms the basis of frequency style thinking



## Law of large numbers in action
```{r, fig.height=5, fig.width=5}
n <- 10000; means <- cumsum(rnorm(n)) / (1  : n); library(ggplot2)
g <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) 
g <- g + geom_hline(yintercept = 0) + geom_line(size = 2) 
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
```



## Law of large numbers in action, coin flip
```{r, fig.height=5, fig.width=5}
means <- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)
g <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) 
g <- g + geom_hline(yintercept = 0.5) + geom_line(size = 2) 
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
```




## Discussion
- An estimator is **consistent** if it converges to what you want to estimate
  - The LLN says that the sample mean of iid sample is
  consistent for the population mean
  - Typically, good estimators are consistent; it's not too much to ask that if we go to the trouble of collecting an infinite amount of data that we get the right answer
- The sample variance and the sample standard deviation
of iid random variables are consistent as well



## The Central Limit Theorem

- The **Central Limit Theorem** (CLT) is one of the most important theorems in statistics
- For our purposes, the CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases
- The CLT applies in an endless variety of settings
- The result is that 
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=
\frac{\sqrt n (\bar X_n - \mu)}{\sigma}
= \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}$$ has a distribution like that of a standard normal for large $n$.
- (Replacing the standard error by its estimated value doesn't change the CLT)
- The useful way to think about the CLT is that 
$\bar X_n$ is approximately
$N(\mu, \sigma^2 / n)$





## Example

- Simulate a standard normal random variable by rolling $n$ (six sided)
- Let $X_i$ be the outcome for die $i$
- Then note that $\mu = E[X_i] = 3.5$
- $Var(X_i) = 2.92$ 
- SE $\sqrt{2.92 / n} = 1.71 / \sqrt{n}$
- Lets roll $n$ dice, take their mean, subtract off 3.5,
and divide by $1.71 / \sqrt{n}$ and repeat this over and over



## Result of our die rolling experiment

```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71
dat <- data.frame(
  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```



## Coin CLT

 - Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin
- The sample proportion, say $\hat p$, is the average of the coin flips
- $E[X_i] = p$ and $Var(X_i) = p(1-p)$
- Standard error of the mean is $\sqrt{p(1-p)/n}$
- Then
$$
    \frac{\hat p - p}{\sqrt{p(1-p)/n}}
$$
will be approximately normally distributed
- Let's flip a coin $n$ times, take the sample proportion
of heads, subtract off .5 and multiply the result by
$2 \sqrt{n}$ (divide by $1/(2 \sqrt{n})$)


## Simulation results
```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5) 
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```


## Simulation results, $p = 0.9$
```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 2)
g + facet_grid(. ~ size)
```


## Galton's quincunx 

http://en.wikipedia.org/wiki/Bean_machine#mediaviewer/File:Quincunx_(Galton_Box)_-_Galton_1889_diagram.png

<img src="fig/quincunx.png" height="450"></img>



## Confidence intervals

- According to the CLT, the sample mean, $\bar X$, 
is approximately normal with mean $\mu$ and sd $\sigma / \sqrt{n}$
- $\mu + 2 \sigma /\sqrt{n}$ is pretty far out in the tail
(only 2.5% of a normal being larger than 2 sds in the tail)
- Similarly, $\mu - 2 \sigma /\sqrt{n}$ is pretty far in the left tail (only 2.5% chance of a normal being smaller than 2 sds in the tail)
- So the probability $\bar X$ is bigger than $\mu + 2 \sigma / \sqrt{n}$
or smaller than $\mu - 2 \sigma / \sqrt{n}$ is 5%
    - Or equivalently, the probability of being between these limits is 95%
- The quantity $\bar X \pm 2 \sigma /\sqrt{n}$ is called
a 95% interval for $\mu$
- The 95% refers to the fact that if one were to repeatly
get samples of size $n$, about 95% of the intervals obtained
would contain $\mu$
- The 97.5th quantile is 1.96 (so I rounded to 2 above)
- 90% interval you want (100 - 90) / 2 = 5% in each tail 
  - So you want the 95th percentile (1.645)



## Give a confidence interval for the average height of sons
in Galton's data
```{r}
library(UsingR);data(father.son); x <- father.son$sheight
(mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12
```



## Sample proportions

- In the event that each $X_i$ is $0$ or $1$ with common success probability $p$ then $\sigma^2 = p(1 - p)$
- The interval takes the form
$$
    \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}
$$
- Replacing $p$ by $\hat p$ in the standard error results in what is called a Wald confidence interval for $p$
- For 95% intervals
$$\hat p \pm \frac{1}{\sqrt{n}}$$ 
is a quick CI estimate for $p$


## Example
* Your campaign advisor told you that in a random sample of 100 likely voters,
  56 intent to vote for you. 
  * Can you relax? Do you have this race in the bag?
  * Without access to a computer or calculator, how precise is this estimate?
* `1/sqrt(100)=0.1` so a back of the envelope calculation gives an approximate 95% interval of `(0.46, 0.66)`
  * Not enough for you to relax, better go do more campaigning!
* Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.
```{r}
round(1 / sqrt(10 ^ (1 : 6)), 3)
```




## Binomial interval

```{r}
.56 + c(-1, 1) * qnorm(.975) * sqrt(.56 * .44 / 100)
binom.test(56, 100)$conf.int
```



## Simulation

```{r}
n <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage <- sapply(pvals, function(p){
  phats <- rbinom(nosim, prob = p, size = n) / n
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})

```



## Plot of the results (not so good)
```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
````


## What's happening?
- $n$ isn't large enough for the CLT to be applicable
for many of the values of $p$
- Quick fix, form the interval with 
$$
\frac{X + 2}{n + 4}
$$
- (Add two successes and failures, Agresti/Coull interval)


## Simulation
First let's show that coverage gets better with $n$

```{r}
n <- 100; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage2 <- sapply(pvals, function(p){
  phats <- rbinom(nosim, prob = p, size = n) / n
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})

```


## Plot of coverage for $n=100$
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage2)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)
```


## Simulation
Now let's look at $n=20$ but adding 2 successes and failures
```{r}
n <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage <- sapply(pvals, function(p){
  phats <- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})
```



## Adding 2 successes and 2 failures
(It's a little conservative)
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)
````



## Poisson interval
* A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
* $X \sim Poisson(\lambda t)$.
* Estimate $\hat \lambda = X/t$
* $Var(\hat \lambda) = \lambda / t$ 
* $\hat \lambda / t$ is our variance estimate


## R code
```{r}
x <- 5; t <- 94.32; lambda <- x / t
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
poisson.test(x, T = 94.32)$conf
```



## Simulating the Poisson coverage rate
Let's see how this interval performs for lambda
values near what we're estimating
```{r}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 100
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
```




## Covarage
(Gets really bad for small values of lambda)
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)
````




## What if we increase t to 1000?
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 1000
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)
```



## Summary
- The LLN states that averages of iid samples 
converge to the population means that they are estimating
- The CLT states that averages are approximately normal, with
distributions
  - centered at the population mean 
  - with standard deviation equal to the standard error of the mean
  - CLT gives no guarantee that $n$ is large enough
- Taking the mean and adding and subtracting the relevant
normal quantile times the SE yields a confidence interval for the mean
  - Adding and subtracting 2 SEs works for 95% intervals
- Confidence intervals get wider as the coverage increases
(why?)
- Confidence intervals get narrower with less variability or
larger sample sizes
- The Poisson and binomial case have exact intervals that
don't require the CLT
  - But a quick fix for small sample size binomial calculations is to add 2 successes and failures
