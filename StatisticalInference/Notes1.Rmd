---
title: "Notes1 - Statistical Inference"
author: "Linda Kukolich"
date: "February 4, 2015"
output: html_document
---
Lecturer: Brian Caffo

These are new lectures with hand written text on them. Pretty, but not the lecture notes from last time. Also more interesting, so far. I will be taking notes, as usual, rather than trying to work from the old notes that are posted. I am basing my notes on the subtitle transcript.

# 1- Introduction

## Statistical Inference Defined
> Generating conclusions about a population from a noisy sample

- Without statistical inference, simply living within our data
- With statistical inference, we're trying to generate new knowledge
    - extend beyond our data to a population to get answers
    - estimators have actual things they are estimating
- Statistical inference is the only formal system of inference we have

## Examples

1. Who will win an election on Election Day based on a sample of likely voters taken today?
    - our sample is a noisy data set.
    - Some people might not actually vote on Election Day,
    - some people might change their mind as to what they're going to say now, or
    - some people might even be deliberately misleading.
    - We'd like to try to draw together all that uncertainly, and use that to predict who's going to win on Election Day.
2. What is the probability it will rain tomorrow?
    - they're trying to use the historical data
    - and particularly the most recent data
    - to predict tomorrow's weather
    - and actually attach a probability to it.
3. What brain regions will light up in an fMRI scanner?
    - Scan people doing a task (tapping fingers)
    - compare +finger-tapping to -finger-tapping
    - figure out what brain regions are associated with finger tapping

- Statistical Inference is Challenging
- Models of randomness are key

## Frequentist Versus Bayesian

- More divides than just that Frequentist versus Bayesian
    - among frequentists
    - among Bayesians
- Choosing frequentist
    - most commonly taught
    - most likely that we are familiar with this
    - most likely that we will work with in future

## Frequentist Thinking

- Probability is like gambling
    - flip a coin an infinite number of times
    - the proportion of heads is the probability of heads
        - $\hat{p} = p$
    - we can repeat the experiment over and over

## Uses of Statistical Inference

- Infer causation (not just association) from noisy statistical data
    - Does smoking cause cancer?
    - Does an ad campaign cause an increase in web traffic?
    - Does a new treatment really return a reduction in cancer progress?
- Requirements enumeration of
    - assumptions
    - set of tools
    - statistical study designs
- Requires us to define what we mean by something causing something else

## Welcome

- this is deep stuff and unusual
    - 4 week course
    - okay to take it twice if it is too hard
- grading
    - 4 homework sets
        - On github
        - harder than quizzes
        - linked to quiz questions
    - 4 quizzes
    - 1 project
        - Think long about project
        - sampling distributions
        - frequency stle statistical inference

- This is Caffo's favorite subject
- He has studied it for 15 years
- He continues to learn new things about it

## Reference Page

- There is a glossary of terms for each week of the class

# 2- Introduction to Probability

- This is a light introduction
- Deeper intro in Bio-stats Bootcamp

## probability

- Given a random experiment (say rolling a die) a probability measure is a **population** quantity that summarizes the randomness
    - The probability is a fixed property of the die
    - Not a simple set of fixed rolls of the die
- Probability takes a possible outcome from the experiment and:
    - assigns it a number of 0 and 1
    - the probability of the union of all possible outcomes is 1 (add all the probabilities)
- Rules
    - The probability that nothing happens is 0
    - The probability that something happens is 1
    - The probability of something is 1 minus the probability of its opposite
    - The probability of at least one of two or more things that cannot simultaneously occur (mutually exclusive), is the sum of their respective probabilities.
    - If the event A implies the occurrence of the event B, then the probability of A occurring is less than the probability that B occurs. (If A is inside B, it can be no larger than B)
    -  For any two events, the probability that at least one occurs is the sum of their probabilities minus their intersection. (If part of A covers part of B, then the area of both is A + B - the part where they overlap)
    
## Example

The [National Sleep Foundation](www.sleepfoundation.org) reports that around 3% of the American population has sleep apnea. They also report around 10% of the North American and European population has restless leg syndrome. *Let's assume, for the sake of argument, that these are probabilities from the same population.* Can we just simply add these probabilities, and conclude that about 13% of people have at least one sleep problem of these sorts in this population?

$$
\begin{eqnarray*}
    A_1 & = & \{\mbox{Person has sleep apnea}\} \\
    A_2 & = & \{\mbox{Person has RLS}\} 
  \end{eqnarray*}
$$
Then 

$$
\begin{eqnarray*}
    P(A_1 \cup A_2 ) & = & P(A_1) + P(A_2) - P(A_1 \cap A_2) \\
   & = & 0.13 - \mbox{Probability of having both}
  \end{eqnarray*}
$$
- No, likely, some fraction of the population has both.

# Probability mass functions

- Random variable
    - numerical outcome of an experiment
    - discrete integer
        - result of a die roll
        - sortable category like month
        - unsortable category like hair color
    - continuous
        - real number like temperature
- PMF - Probability Mass Function
    - a probability mass function evaluated at a value corresponds to the probability that a random variable takes that value.
    - PMF is &ge; 0
    - sum of PMF for all possible values is 1

## Example - coin flip

- X = 0 - tails
- X = 1 - heads
- Fair coin
    - $p(x) = (1/2)^x(1/2)^{1-x}$ for x $\in$ 0,1
- Unfair coin
    - $p(x) = \theta^x(1-\theta)^{1-x}$ for x $\in$ 0, 1

# Probability density functions

- A probability density function (pdf) is a function associated with a continuous random variable
- To be a valid pd, a function must satisfy
    - it must be larger than zero everywhere
    - the total area under the curve must be 1
- Areas under pdfs correspond to probabilities for that random variable
- probability of a *specific* value is 0, because you need a width to make an area

## Example

$$
f(x) =
\begin{cases}
2x & \text{for } & 0 < x < 1\\
0  & \text{otherwise } &
\end{cases}
$$

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")
# Valid?
(1*2)/2
```

What is the probability that 75% of less calls are handled in one day?

```{r, fig.height = 5, fig.width = 5, echo = FALSE, fig.align='center'}
plot(x, y, lwd = 3, frame = FALSE, type = "l")
polygon(c(0, .75, .75, 0), c(0, 0, 1.5, 0), lwd = 3, col = "lightblue")
```

```{r}
(2*.75 * .75)/2
pbeta(0.75, 2, 1)
```


## CDF and survival function
### Certain areas are so useful, we give them names

- The **cumulative distribution function** (CDF) of a random variable, $X$, returns the probability that the random variable is less than or equal to the value $x$
$$
F(x) = P(X \leq x)
$$

```{r}
# From the swirl work
# Get the CDF for a PDF of my creation (actually, theirs)
mypdf <- function(x){ x/2}
integrate(mypdf, lower=0, upper=1.6)
```

(This definition applies regardless of whether $X$ is discrete or continuous.)
- The **survival function** of a random variable $X$ is defined as the probability
that the random variable is greater than the value $x$
$$
S(x) = P(X > x)
$$
- Notice that $S(x) = 1 - F(x)$
```{r}
# Calculate the survival function for mypdf
integrate(mypdf, lower=1.6, upper=2)
```

---

## Example

What are the survival function and CDF from the density considered before?

For $1 \geq x \geq 0$
$$
F(x) = P(X \leq x) = \frac{1}{2} Base \times Height = \frac{1}{2} (x) \times (2 x) = x^2
$$

$$
S(x) = 1 - x^2
$$

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

---

## Quantiles

You've heard of sample quantiles. If you were the 95th percentile on an exam, you know
that 95% of people scored worse than you and 5% scored better. 
These are sample quantities. Here we define their population analogs.


---
## Definition

The  $\alpha^{th}$ **quantile** of a distribution with distribution function $F$ is the point $x_\alpha$ so that
$$
F(x_\alpha) = \alpha
$$

*Draw the pdf, or line up an infinite number of test scores. Find the point were 1-alpha percent of the population scores this well or lower and alpha percent scores higher.*

- A **percentile** is simply a quantile with $\alpha$ expressed as a percent
- The **median** is the $50^{th}$ percentile

---
## For example

The $95^{th}$ percentile of a distribution is the point so that:
- the probability that a random variable drawn from the population is less is 95%
- the probability that a random variable drawn from the population is more is 5%

---
## Example
What is the median of the distribution that we were working with before?
- We want to solve $0.5 = F(x) = x^2$
- Resulting in the solution 
```{r, echo = TRUE} 
sqrt(0.5)
``` 
- Therefore, about `r sqrt(0.5)` of calls being answered on a random day is the median.

---
## Example continued
R can approximate quantiles for you for common distributions

```{r}
qbeta(0.5, 2, 1)
```

---

## Summary

- You might be wondering at this point "I've heard of a median before, it didn't require integration. Where's the data?"
- We're referring to are **population quantities**. Therefore, the median being
  discussed is the **population median**.
- A probability model connects the data to the population using assumptions.
- Therefore the median we're discussing is the **estimand**, the sample median will be the **estimator**


# 3- Conditional Probability

## Conditional probability, motivation

- The probability of getting a one when rolling a (standard) die is usually assumed to be one sixth
- Suppose you were given the extra information that the die roll was an odd number (hence 1, 3 or 5)
- *conditional on this new information*, the probability of a one is now one third


- Let $B$ be an event so that $P(B) > 0$
- Then the conditional probability of an event $A$ given that $B$ has occurred is
  $$
  P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}
  $$
- Notice that if $A$ and $B$ are independent (defined later in the lecture), then
  $$
  P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)
  $$

## Example

- Consider our die roll example
- $B = \{1, 3, 5\}$
- $A = \{1\}$
$$
  \begin{eqnarray*}
P(\mbox{one given that roll is odd})  & = & P(A ~|~ B) \\ \\
  & = & \frac{P(A \cap B)}{P(B)} \\ \\
  & = & \frac{P(A)}{P(B)} \\ \\ 
  & = & \frac{1/6}{3/6} = \frac{1}{3}
  \end{eqnarray*}
$$

# Bayes' rule
Baye's rule allows us to reverse the conditioning set provided that we know some marginal probabilities
(Presbyterian Minister, Thomas Bayes)
$$
P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}.
$$

## Diagnostic tests

- Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
- Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively 
- The **sensitivity** is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
- The **specificity** is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$

## More definitions

- The **positive predictive value** is the probability that the subject has the  disease given that the test is positive, $P(D ~|~ +)$
- The **negative predictive value** is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
- The **prevalence of the disease** is the marginal probability of disease, $P(D)$
- The **diagnostic likelihood ratio of a positive test**, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
- The **diagnostic likelihood ratio of a negative test**, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$

## Example

- A study comparing the efficacy of HIV tests, reports on an experiment which concluded that HIV antibody tests have a sensitivity of 99.7% and a specificity of 98.5% (*Made up numbers*)
- Suppose that a subject, from a population with a .1% prevalence of HIV, receives a positive test result. What is the positive predictive value?
- Mathematically, we want $P(D ~|~ +)$ given the sensitivity, $P(+ ~|~ D) = .997$, the specificity, $P(- ~|~ D^c) =.985$, and the prevalence $P(D) = .001$


## Using Bayes' formula

$$
\begin{eqnarray*}
  P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\ \\
 & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\ \\
 & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\ \\
 & = & .062
\end{eqnarray*}
$$

```{r}
(.997*.001)/(.997 * .001 + (1-.985)*(1-.001))
```

- In this population a positive test result only suggests a 6% probability that the subject has the disease 
- (The positive predictive value is 6% for this test)

## More on this example

- The low positive predictive value is due to low prevalence of disease and the somewhat modest specificity
- Suppose it was known that the subject was an intravenous drug user and routinely had intercourse with an HIV infected partner
- Notice that the evidence implied by a positive test result does not change because of the prevalence of disease in the subject's population, only our interpretation of that evidence changes

## Likelihood ratios

- Using Bayes rule, we have
  $$
  P(D ~|~ +) = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)} 
  $$
  and
  $$
  P(D^c ~|~ +) = \frac{P(+~|~D^c)P(D^c)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}.
  $$

---

## Likelihood ratios

- Therefore
$$
\frac{P(D ~|~ +)}{P(D^c ~|~ +)} = \frac{P(+~|~D)}{P(+~|~D^c)}\times \frac{P(D)}{P(D^c)}
$$
ie
$$
\mbox{post-test odds of }D = DLR_+\times\mbox{pre-test odds of }D
$$
- Similarly, $DLR_-$ relates the decrease in the odds of the
  disease after a negative test result to the odds of disease prior to
  the test.

---

## HIV example revisited

- Suppose a subject has a positive HIV test
- $DLR_+ = .997 / (1 - .985) \approx 66$
- The result of the positive test is that the odds of disease is now 66 times the pretest odds
- Or, equivalently, the hypothesis of disease is 66 times more supported by the data than the hypothesis of no disease

---

## HIV example revisited

- Suppose that a subject has a negative test result 
- $DLR_- = (1 - .997) / .985  \approx .003$
- Therefore, the post-test odds of disease is now $.3\%$ of the pretest odds given the negative test.
- Or, the hypothesis of disease is supported $.003$ times that of the hypothesis of absence of disease given the negative test result

# Independence

- Two events $A$ and $B$ are **independent** if $$P(A \cap B) = P(A)P(B)$$
- Equivalently if $P(A ~|~ B) = P(A)$ 
- Two random variables, $X$ and $Y$ are independent if for any two sets $A$ and $B$ $$P([X \in A] \cap [Y \in B]) = P(X\in A)P(Y\in B)$$
- If $A$ is independent of $B$ then 
  - $A^c$ is independent of $B$ 
  - $A$ is independent of $B^c$
  - $A^c$ is independent of $B^c$


---

## Example

- What is the probability of getting two consecutive heads?
- $A = \{\mbox{Head on flip 1}\}$ ~ $P(A) = .5$
- $B = \{\mbox{Head on flip 2}\}$ ~ $P(B) = .5$
- $A \cap B = \{\mbox{Head on flips 1 and 2}\}$
- $P(A \cap B) = P(A)P(B) = .5 \times .5 = .25$ 

---

## Example

- Volume 309 of Science reports on a physician who was on trial for expert testimony in a criminal trial
- Based on an estimated prevalence of sudden infant death syndrome of $1$ out of $8,543$, the physician testified that that the probability of a mother having two children with SIDS was $\left(\frac{1}{8,543}\right)^2$
- The mother on trial was convicted of murder

---

## Example: continued

- Relevant to this discussion, the principal mistake was to *assume* that the events of having SIDs within a family are independent
- That is, $P(A_1 \cap A_2)$ is not necessarily equal to $P(A_1)P(A_2)$
- Biological processes that have a believed genetic or familiar environmental component, of course, tend to be dependent within families
- (There are many other statistical points of discussion for this case.)


---
## IID random variables

- Random variables are said to be iid if they are independent and identically distributed
  - Independent: statistically unrelated from one and another
  - Identically distributed: all having been drawn from the same population distribution
- iid random variables are the default model for random samples
- Many of the important theories of statistics are founded on assuming that variables are iid
- Assuming a random sample and iid will be the default starting point of inference for this class



# 4- Expected Values

- Expected values are useful for characterizing a distribution
- The mean is a characterization of its center
- The variance and standard deviation are characterizations of
how spread out it is
- Our sample expected values (the sample mean and variance) will
estimate the population versions


---
## The population mean
- The **expected value** or **mean** of a random variable is the center of its distribution
- For discrete random variable $X$ with PMF $p(x)$, it is defined as follows
    $$
    E[X] = \sum_x xp(x).
    $$
    where the sum is taken over the possible values of $x$
- $E[X]$ represents the center of mass of a collection of locations and weights, $\{x, p(x)\}$

---
## The sample mean
- The sample mean estimates this population mean
- The center of mass of the data is the empirical mean
$$
\bar X = \sum_{i=1}^n x_i p(x_i)
$$
where $p(x_i) = 1/n$

---

## Example
### Find the center of mass of the bars
```{r galton, fig.height=6,fig.width=12, fig.align='center', echo = FALSE, message =FALSE, warning=FALSE}
library(UsingR); data(galton); library(ggplot2)
library(reshape2)
longGalton <- melt(galton, measure.vars = c("child", "parent"))
g <- ggplot(longGalton, aes(x = value)) + geom_histogram(aes(y = ..density..,  fill = variable), binwidth=1, colour = "black") + geom_density(size = 2)
g <- g + facet_grid(. ~ variable)
g
```

---
## Using manipulate
```
library(manipulate)
myHist <- function(mu){
    g <- ggplot(galton, aes(x = child))
    g <- g + geom_histogram(fill = "salmon", 
      binwidth=1, aes(y = ..density..), colour = "black")
    g <- g + geom_density(size = 2)
    g <- g + geom_vline(xintercept = mu, size = 2)
    mse <- round(mean((galton$child - mu)^2), 3)  
    g <- g + labs(title = paste('mu = ', mu, ' MSE = ', mse))
    g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```

---
## The center of mass is the empirical mean
```{r lsm, dependson="galton",fig.height=7,fig.width=7, fig.align='center', echo = FALSE}
    g <- ggplot(galton, aes(x = child))
    g <- g + geom_histogram(fill = "salmon", 
      binwidth=1, aes(y = ..density..), colour = "black")
    g <- g + geom_density(size = 2)
    g <- g + geom_vline(xintercept = mean(galton$child), size = 2)
    g
```


---
## Example of a population mean

- Suppose a coin is flipped and $X$ is declared $0$ or $1$ corresponding to a head or a tail, respectively
- What is the expected value of $X$? 
    $$
    E[X] = .5 \times 0 + .5 \times 1 = .5
    $$
- Note, if thought about geometrically, this answer is obvious; if two equal weights are spaced at 0 and 1, the center of mass will be $.5$

```{r, echo = FALSE, fig.height=4, fig.width = 6, fig.align='center'}
ggplot(data.frame(x = factor(0 : 1), y = c(.5, .5)), aes(x = x, y = y)) + geom_bar(stat = "identity", colour = 'black', fill = "lightblue")
```

---
## What about a biased coin?

- Suppose that a random variable, $X$, is so that
$P(X=1) = p$ and $P(X=0) = (1 - p)$
- (This is a biased coin when $p\neq 0.5$)
- What is its expected value?
$$
E[X] = 0 * (1 - p) + 1 * p = p
$$

---

## Example

- Suppose that a die is rolled and $X$ is the number face up
- What is the expected value of $X$?
    $$
    E[X] = 1 \times \frac{1}{6} + 2 \times \frac{1}{6} +
    3 \times \frac{1}{6} + 4 \times \frac{1}{6} +
    5 \times \frac{1}{6} + 6 \times \frac{1}{6} = 3.5
    $$
- Again, the geometric argument makes this answer obvious without calculation.

```{r, fig.align='center', echo=FALSE, fig.height=4, fig.width=10}
ggplot(data.frame(x = factor(1 : 6), y = rep(1/6, 6)), aes(x = x, y = y)) + geom_bar(stat = "identity", colour = 'black', fill = "lightblue")

```

```{r}
expect_dice <- function(pmf) {
    mu <- 0
    for(i in 1:6) mu <- mu + i*pmf[i];
    mu
}
# dice_high, 6 rolls, E = 4.333
dice_high <- c(0.04761905, 0.09523810, 0.14285714, 0.19047619, 0.23809524, 0.28571429)
edh <- expect_dice(dice_high)
# dice_low, 6 rolls, E = 2.667
dice_low <- rev(dice_high)
edl <- expect_dice(dice_low)
edh + edl
```

---

## Continuous random variables

- For a continuous random variable, $X$, with density, $f$, the expected value is again exactly the center of mass of the density


---

## Example

- Consider a density where $f(x) = 1$ for $x$ between zero and one
- (Is this a valid density?)
- Suppose that $X$ follows this density; what is its expected value?  
```{r, fig.height=6, fig.width=6, echo=FALSE, fig.align='center'}
g <- ggplot(data.frame(x = c(-0.25, 0, 0, 1, 1, 1.25),
                  y = c(0, 0, 1, 1, 0, 0)),
       aes(x = x, y = y))
g <- g + geom_line(size = 2, colour = "black")
g <- g + labs(title = "Uniform density")
g  

```

```{r}
# mean(Mean of 2 samples) = mean(original set)
spop <- c(1, 4, 7, 10, 13)
allsam <- t(combn(spop, 2))
smeans <- apply(allsam, 1, mean)
mean(smeans)
```

---

## Facts about expected values

- Recall that expected values are properties of distributions
- Note the average of random variables is itself a random variable
and its associated distribution has an expected value
- The center of this distribution is the same as that of the original distribution
- Therefore, the expected value of the **sample mean** is the population mean that it's trying to estimate
- When the expected value of an estimator is what its trying to estimate, we say that the estimator is **unbiased**
- Let's try a simulation experiment

---
## Simulation experiment
Simulating normals with mean 0 and variance 1 versus averages
of 10 normals from the same population

```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
library(ggplot2)
nosim <- 10000; n <- 10
dat <- data.frame(
    x = c(rnorm(nosim), apply(matrix(rnorm(nosim * n), nosim), 1, mean)),
    what = factor(rep(c("Obs", "Mean"), c(nosim, nosim))) 
    )
ggplot(dat, aes(x = x, fill = what)) + geom_density(size = 2, alpha = .2); 

```

---
## Averages of x die rolls

```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}  
dat <- data.frame(
  x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(1 : 4, rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = "black") 
g + facet_grid(. ~ size)
```


---
## Averages of x coin flips
```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
dat <- data.frame(
  x = c(sample(0 : 1, nosim, replace = TRUE),
        apply(matrix(sample(0 : 1, nosim * 10, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 20, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 30, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(c(1, 10, 20, 30), rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth = 1 / 12, colour = "black"); 
g + facet_grid(. ~ size)
```

---
## Summarizing what we know
- Expected values are properties of distributions
- The population mean is the center of mass of population
- The sample mean is the center of mass of the observed data
- The sample mean is an estimate of the population mean
- The sample mean is unbiased 
  - The population mean of its distribution is the mean that it's
  trying to estimate
- The more data that goes into the sample mean, the more 
concentrated its density / mass function is around the population mean
