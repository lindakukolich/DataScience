---
title: "Notes1"
author: "Linda Kukolich"
date: "January 9, 2015"
output: html_document
---
# Reproducible Research, Week 1 #
Taught by Roger Peng, Associate Professor of Biostatistics at Johns Hopkins Bloomberg School of Public Health. Will talk about reproducible research, investigate tools that support it, and will conclude with case studies where research succeeded or failed based on its reproducibility.

# Reproducible Research: Concepts and Ideas #
The ultimate standard for strengthening scientific evidence is replication of findings and conducting studies with independent

* investigators
* data
* analytical methods
* laboratories
* instruments

It can be diffiult to replicate a study (no time, no money, unique opportunity). Instead of replicating the study, reinvestigate the data that is produced. 

Replication is particularly important in studies that can impact broad policy or regulatory decisions. Reproducible research makes analytic data dn code available so that others may reporduce findings.

Why we need reproducible research

* New technologies increasing data collection througput; data are more complex and extremely high dimensional
* Existing databases can be merged into new "megadatabases"
* Computing power is greatly increased, allowing more sophisticated analyses
* For every field "X" there is a field "Computational X"

Example: Reproducible Air Pollution and Health Research

Johns Hopkins hosts a collection of Pollution related data bases and analyses, *Inernet-based Health and Air Pollution Surveillance System*

* Estimating small (but important) health effects in the presence of much stronger signals (pollution hurts you, but something else kills you)
* Results inform substantial policy decisions, affect many stakeholders
* Complex statistical methods are needed and subjected to intense scrutiny

Research Pipeline

* Measured Data, plus Processing code produces
* Analytic Data, which, plus Analytic code produces
* Computational Results, which, plus Presentation code produeces
    * Figures
    * Tables
    * Numerical Summaries
* which, are all combined with Text to produce an Article

What we need

* Analytic data are available
* Analytic code are available
* Documentation of code and data
* Standard means of distribution
* Authors who want to make their research reproducible and need tools for that
* Readers who want to reproduce and possibly expand upon interesting findings, and also need tools

Challenges

* It can be hard for authors to put data/results on the web
* It can be hard to piece together the original data
* Readers may not have the same resources as the authors
* There are not that many tools available

Literate (Statistical) Programming

* Literate programming is a general concept that requires
    1. A (human readable) documentation language
    2. A (machine readable) programming language
* Sweave uses Latex and R as the documentation and programming langes
    * Developed by Friedrich Leisch
    * Maintained by R Core
    * [http://www.statistik.lmu.de/~leisch/Sweave]
    * Limitations
        * Latex is hard to use
        * Plotting is hard
        * Not frequently updated or actively developed
* knitr uses R and a variety of documentation languages (Latex, Markdown, HTML)
    * Started where Sweave left off, addressing its limitations
    * Developed by Yihui Xie while at Iowa State
    * [http://yihui.name/knitr/]

# First Principle of Reproducible Research

Script everything. This is like the old lab notebooks, but since everything happens in computers now-days, record the computer command that accomplish the task.
# Structure of a Data Analysis #

1. **Define the question**
    * start with a general question *Can I automatically detect emals that are SPAM and that are not?*
    * Make it concrete *Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?*
2. **Define the ideal data set** which may depend on your goal
    * Descriptive - a whole population
    * Exploratory - a random sample with many variables measured
    * Inferential - the right population, randomly sampled
    * Predictive - a training and test data set from the same population
    * Causal - data from a randomized study
    * Mechanistic - data about all components of the system
    * *For our example, all of gmail, which would be great but they aren't likely to hand it out*
3. **Determine what data you can access**
    * Sometimes uou can find data free on the web
    * Other times you may need to buy the data
    * Be sure to respect the terms of use
    * If the data don't exist, you may need to generate it yourself
    * *There is a spam data set at UCI Machine Learning Repository*
    * *It is included in the kerlab package*
4. **Obtain the data**
    * Try to obtain the raw data
    * Be sure to reference the source
    * Polite emails go a long way
    * If you will load the data from an internet source, record the url and time accessed
5. **Clean the data**
    * Raw data often needs to be processed
    * If it is pre-processed, make sure you understand how
    * Understand the source of the data (census, sample, convience sample, etc.)
    * May need reformatting, subsampling - record these steps
    * **Determine if the data are good enough** - if not, quit or change data
6. **Exploratory data analysis**
    * For our question (SPAM-ID) we need a training set (a training parameter evaluation set) and a testing set (seen only once). Well, that's if I'm in charge...
    * `rbinom()` can give us random assignments to training and testing sets
    * Steps
        1. Look at summaries of the data
            * `names(trainSpam)`
            * `head(trainSpam)`
            * `table(trainSpam$type)`
        2. Check for missing data
        3. Create exploratory plots
            * `plot(trainSpam$capitalAve ~ trainSpam$type)`
            * `plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)`
            * `plot(log10(trainSpam[, 1:4] + 1))` # lattice of 4 fields
        4. Perform exploratory analyses (e.g. clustering)
            * `hCluster = hcluster(dist(t(trainSpam[, 1:57])))`
            * `plot(hCluster) # Cluster dendrogram`
7. Statistical prediction/modeling
    * Should be informed by the results of your exploratory analysis
    * Exact methods depend on the question of interest
    * Transformations / processing should be accounted for whem neccessary.
    * Measures of uncertainty should be reported
```
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y > 0.5))
cvError = rep(NA, 55)
library(boot)
for(i in 1:55) {
  lmFormula = reformulate(names(trainSpam)[i], response = "numType")
   glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
  cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
# Which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)] # charDollar
# Get a measure of uncertainty
predictionModel = glm(numType ~ charDollar, family = "binomial", data= trainSpam)
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam", dim(testSpam)[1])
predictedSpam[predictionModel$fitted > 0.5] = "spam"
table(predictedSpam, testSpam$type) # really spam vs predicted
# predictedSpam nonspam spam
#      nonspam    1346  458
#         spam      61  449
# Error rate - incorrect over total
(61 + 458)/(1346 + 458 + 61 + 449)
```

8. Interpret results
    * Use appropriate language
        * describes
        * correlates with/associated with
        * leads to/causes
        * predicts
    * Give an explanation
    * Interpret coefficients
    * Interpret measures of uncertainty
```    
The fraction of characters that are dollar signs can be used to predict if email is Spam
Anything with more than 6.6% dollar signs is classified as Spam
More dollar signs always means more Spam under our prediction
Our test set error rate was 22.4%
```
9. Challenge results
    * Challenge all steps
      * Question
      * Data source
      * Processing
      * Analysis
      * Conclusions
    * Challenge measures of uncertainty
    * Challenge choices of terms to include in models
    * Think of potential alternative analyses
10. Synthesize/write up results
  * Lead with the question
  * Summarize the analyses into the story
  * Don't include every analysis, include it
    * if it is needed for the story
    * if it is needed to address a challenge
  * order analyses according to the story, rather than chronologically
  * include "pretty" figures that contribute to the story
  * Our example
    * Lead with the question
      * Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
    * Describe the approach
      * Collected data from UCI -> created training/test sets
      * Explored relationships
      * Choose logistic model on training set by cross validation
      * Applied to test, 78% test set accuracy
    * Interpret results
      * Number of dollar signs seems reasonable, e.g. "Make money with Viagra $ $ $ $"
    * Challenge results
      * 78% isn't that great
      * I could use more variables
      * Why logistic regression?
11. Create reproducible code
  * Showed an Rmd document with R code blocks for most of the example code above. Actually, it is probably the lecture notes we are seeing now...

# Organizing a Data Analysis #

Data analysis files

* Data
    * raw data
        * Should be stored in your analysis folder
        * If accessed from the web, include url, description, and date accessed in README
    * processed data
        * Processed data should be named so it is easy to see which script generated the data
        * The processing script - processed data mapping should occur in the README
        * Processed data should be **tidy**
* Figures
    * exploratory figures
        * Figures made during the course of your analysis, not necessarily part of your final report
        * They do not need to be "pretty"
    * final figures
        * Usually a small subset of the orginal figures
        * Axes/colors set to make the figure clear
        * possibly multiple panels
* R code
    * raw / unused scripts
        * May be less commented
        * May be multiple versions
        * May include analyses that are later discarded
    * final scripts
        * Clearly commented
            * small comments liberally - what, when, why, how
            * bigger commented block for whole sections
        * include processing details
        * only analyses that appear in the final write-up
    * R markdown files
        * R markdown files can be use to generate reproducible reports
        * text and R code are integrated
        * very easy to create in RStudio
* Text
    * README files
        * Not necessary if you use R markdown
        * *though GitHub looks for them, so I (LCK) disagree*
        * Should contain step-by-step instructions for analysis
        * An example *Stale link*
    * Text of analysis / report
        * It should include a title, introduction (motivation), methods (statistics you used), results (including measures of uncertainty), and conclusions (including potential problems)
        * It should tell a story
        * *It should not include every analysis you performed*
        * References should be included for statistical methods

Resources

* Information about a non-reproducible study that lead to cancer patients being mistreated: Duke Saga Starter Set [http://simplystatistics.org/2012/02/27/the-duke-saga-starter-set/]
* Reproducible resesarch and Biostatistics [http://biostatistics.oxfordjournals.org/content/10/3/405.full]
* Managing a statistical analysis project guidelines and best practices [http://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/]
* Project template - a pre-organized set of files for data analysis [http://projecttemplate.net/]

The project template thing looks really cool. It even includes hooks into Rs unit testing support.
